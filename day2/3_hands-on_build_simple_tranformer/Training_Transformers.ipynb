{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYhPEC3DC5Yg"
   },
   "source": [
    "# Training Transformers from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHEsWuqHDNWp"
   },
   "source": [
    "In this lab, we will discuss how to train a transformer model using PyTorch. Training transformers is not very different from training other types of models. However, we do require some additional helper functions, such as positional encoding, to construct the model. Additionally, we'll need to modify the preprocessing steps to suit our new model.\r\n",
    "\r\n",
    "While creating this tutorial, we referred to some of the official PyTorch tutorial pages. You can find these references in the \"References\" section at the end of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rlNcYZfC2JK",
    "outputId": "6ee9ecf6-e47b-4ac1-9729-f74db446a701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version of the torch:2.0.1\n",
      "version of the torchtext:0.15.2\n",
      "version of the torchdata:0.6.1\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import torch\n",
    "import torchtext\n",
    "from torch import nn\n",
    "import torchdata\n",
    "import math\n",
    "%matplotlib inline\n",
    "print('version of the torch:' + torch.__version__)\n",
    "print('version of the torchtext:' + torchtext.__version__)\n",
    "print('version of the torchdata:' + torchdata.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_A3uN4PP_yl"
   },
   "source": [
    "## nn.Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U_i11PRFp4B"
   },
   "source": [
    "Before we begin, let's discuss the tools that PyTorch provides for training a transformer model. One of the standout features of PyTorch is its modularity, and this extends to the nn.Transformer module, which offers a highly customizable model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKnDm2XuJ39A",
    "outputId": "8d1a7c21-db37-4190-a17a-4d92261145ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(nn.Transformer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmeNb5BvKTJO"
   },
   "source": [
    "As we can see above, simply calling the transformer model automatically sets up both the encoder and decoder layers, complete with their respective functionalities. Of course, these layers can be easily modified by adjusting their input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4seK-OErLVNg",
    "outputId": "6631a8af-fe77-4570-8de3-8e6df023fdbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A transformer model. User is able to modify the attributes as needed. The architecture\n",
      "    is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer,\n",
      "    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\n",
      "    Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\n",
      "    Processing Systems, pages 6000-6010.\n",
      "\n",
      "    Args:\n",
      "        d_model: the number of expected features in the encoder/decoder inputs (default=512).\n",
      "        nhead: the number of heads in the multiheadattention models (default=8).\n",
      "        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n",
      "        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n",
      "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
      "        dropout: the dropout value (default=0.1).\n",
      "        activation: the activation function of encoder/decoder intermediate layer, can be a string\n",
      "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
      "        custom_encoder: custom encoder (default=None).\n",
      "        custom_decoder: custom decoder (default=None).\n",
      "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
      "        batch_first: If ``True``, then the input and output tensors are provided\n",
      "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
      "        norm_first: if ``True``, encoder and decoder layers will perform LayerNorms before\n",
      "            other attention and feedforward operations, otherwise after. Default: ``False`` (after).\n",
      "\n",
      "    Examples::\n",
      "        >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
      "        >>> src = torch.rand((10, 32, 512))\n",
      "        >>> tgt = torch.rand((20, 32, 512))\n",
      "        >>> out = transformer_model(src, tgt)\n",
      "\n",
      "    Note: A full example to apply nn.Transformer module for the word language model is available in\n",
      "    https://github.com/pytorch/examples/tree/master/word_language_model\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(nn.Transformer().__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-L5KwpBLpnf"
   },
   "source": [
    "Let's say you want to create a custom model that replaces the default transformer decoder with an LSTM. This can be easily accomplished by modifying the custom_decoder parameter and supplying a functional LSTM model for it. On the other hand, if you're not looking for such an extreme customization, you can make more targeted adjustments by utilizing the built-in `TransformerDecoderLayer` and `TransformerDecoder` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntt_udOBKSCT",
    "outputId": "9166d499-3b9f-4a1f-c22b-3cf06969a56c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoderLayer(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (dropout3): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "TransformerDecoder(\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x TransformerDecoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (multihead_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (dropout3): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "decoder = nn.TransformerDecoder(decoder_layer,num_layers=3)\n",
    "print(decoder_layer)\n",
    "print('\\n\\n')\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzRulazQOwrd"
   },
   "source": [
    "After creating your new decoder layer we can simly call `Transformer` with new decoder and create new model. Similary Encoder can also be edited by built-in `TransformerEncoderLayer` or `TransformerEncoder` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJcTgVO7NUi8",
    "outputId": "d5ea9696-f3e0-4eaa-c8f2-d89b3931bc8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Transformer(custom_decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fagUPSz0QEY4"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01lqY_6bQH3T"
   },
   "source": [
    "Now that we understand how the Transformer works, we can start building our model. For those of you with keen eyes, you may have noticed that one component is missing from the transformer architecture. Can you guess what it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnMt14uySMrg"
   },
   "source": [
    "### @Spoiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uvj4XsKStEz"
   },
   "source": [
    "Yes it is positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJFJrviJQGFG",
    "outputId": "206e6992-7985-4b63-c0a6-50184c782e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5964, -0.8773,  0.1465,  0.7073],\n",
      "         [-1.5964, -0.8773,  0.1465,  0.7073]],\n",
      "\n",
      "        [[-1.5964, -0.8773,  0.1465,  0.7073],\n",
      "         [-1.5964, -0.8773,  0.1465,  0.7073]],\n",
      "\n",
      "        [[-1.5964, -0.8773,  0.1465,  0.7073],\n",
      "         [-1.5964, -0.8773,  0.1465,  0.7073]],\n",
      "\n",
      "        [[-1.5964, -0.8773,  0.1465,  0.7073],\n",
      "         [-1.5964, -0.8773,  0.1465,  0.7073]],\n",
      "\n",
      "        [[-1.5964, -0.8773,  0.1465,  0.7073],\n",
      "         [-1.5964, -0.8773,  0.1465,  0.7073]],\n",
      "\n",
      "        [[-1.5964, -0.8773,  0.1465,  0.7073],\n",
      "         [-1.5964, -0.8773,  0.1465,  0.7073]]], grad_fn=<MulBackward0>)\n",
      "tensor([[[-1.5964,  0.1227,  0.1465,  1.7073],\n",
      "         [-1.5964,  0.1227,  0.1465,  1.7073]],\n",
      "\n",
      "        [[-0.7549, -0.3370,  0.1565,  1.7072],\n",
      "         [-0.7549, -0.3370,  0.1565,  1.7072]],\n",
      "\n",
      "        [[-0.6871, -1.2934,  0.1665,  1.7071],\n",
      "         [-0.6871, -1.2934,  0.1665,  1.7071]],\n",
      "\n",
      "        [[-1.4553, -1.8673,  0.1765,  1.7068],\n",
      "         [-1.4553, -1.8673,  0.1765,  1.7068]],\n",
      "\n",
      "        [[-2.3532, -1.5309,  0.1865,  1.7065],\n",
      "         [-2.3532, -1.5309,  0.1865,  1.7065]],\n",
      "\n",
      "        [[-2.5553, -0.5936,  0.1965,  1.7060],\n",
      "         [-2.5553, -0.5936,  0.1965,  1.7060]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_size: int,\n",
    "                 dropout: float= 0.1,\n",
    "                 maximum_length: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        divider = torch.exp(- torch.arange(0, embedding_size, 2)* math.log(10000) / embedding_size)\n",
    "        position = torch.arange(0, maximum_length).unsqueeze(1)\n",
    "\n",
    "        positionalembedding = torch.zeros((maximum_length,1, embedding_size))\n",
    "        positionalembedding[:,0, 0::2] = torch.sin(position * divider)\n",
    "        positionalembedding[:,0, 1::2] = torch.cos(position * divider)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('positionalembedding', positionalembedding)\n",
    "\n",
    "    def forward(self, input_token: torch.Tensor):\n",
    "        embedding = input_token + self.positionalembedding[:input_token.size(0), :]\n",
    "        return self.dropout(embedding)\n",
    "\n",
    "\n",
    "dumposition = PositionalEncoding(4,0)\n",
    "dumembedding = nn.Embedding(10, 4)\n",
    "embed = dumembedding(torch.tensor(([1,1,1,1,1,1],[1,1,1,1,1,1]))).transpose(0,1)* 2 # Normally, we use the square root of the embedding size, but for demonstration purposes, we'll multiply the embedding size by 100.\n",
    "embedpos = dumposition(embed)\n",
    "print(embed)\n",
    "print(embedpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b07PLQRZH-U"
   },
   "source": [
    "In the output, we can observe subtle alterations to the model's embeddings, which are a result of the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzM1t1dvaMpz"
   },
   "source": [
    "### Model cont.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TuvqXCZaUZd"
   },
   "source": [
    "Now that we've learned how the model works, let's proceed to create it. Last time, we familiarized ourselves with the AG_News dataset, and we're going to use it again. For this purpose, we'll build a classification model using only the transformer encoder layer. Let's go ahead and fill in the empty spaces within the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-_bU9CTgaRUl"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, embedding_size: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, nclass: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = nn.Embedding(ntoken, embedding_size)\n",
    "\n",
    "        self.PositionalEncoding = PositionalEncoding(embedding_size, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embedding_size, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "\n",
    "        self.linear = nn.Linear(embedding_size, nclass)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding_size)\n",
    "        src = self.PositionalEncoding(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eio-e8lgrcC"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4pd3q5T7wlg"
   },
   "source": [
    "Data processing for this tutorial will be quite similar to our previous approach, with only minor modifications needed to accommodate our new model. These changes may include using regular embeddings instead of an embedding bag, or shifting from a one-to-one model to a many-to-one model, among other adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "B6Iv8-nIgqg6"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "train_iter = iter(AG_NEWS(split=\"train\"))\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\",\"<pad>\"])  # Creates a dictionary for tokens\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "udHpOwwthZx1"
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))   # Pipelines for conversion\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab([\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-qkCA3BhhUX9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "   label_list, text_list = [], []\n",
    "   for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text))\n",
    "        text_list.append(processed_text)\n",
    "   return torch.tensor(label_list).to(device), pad_sequence(text_list, padding_value=vocab([\"<pad>\"])[0]).to(device)\n",
    "# def collate_batch(batch):\n",
    "#     label_list, text_list, offsets = [], [], [0]\n",
    "#     for _label, _text in batch:\n",
    "#         label_list.append(label_pipeline(_label))\n",
    "#         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "#         text_list.append(processed_text)\n",
    "#         offsets.append(processed_text.size(0))\n",
    "#     label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "#     offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "#     text_list = torch.cat(text_list)\n",
    "#     return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RIGaeSAUg1yv"
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 16  # embedding dimension\n",
    "d_hid = 8  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "nclasses = 4\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers,nclasses, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "X4xTx3vYnPaP"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 4\n",
    "LR = 5\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch,drop_last= True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch,drop_last= True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch,drop_last= True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gbxJzUW8Qlm"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mooSSMLY8URx"
   },
   "source": [
    "Once again, we will utilize the training loop that we constructed in the previous lab. The only difference this time is that we'll be taking the last item from the model's output for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qbY7nOJOnplO"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(text)\n",
    "        predicted_label =out[-1]\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) ## To prevent expoding gradient\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            out = model(text)\n",
    "            predicted_label =out[-1]\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p041QuzZnwRa",
    "outputId": "bebb1567-bd19-47da-a986-eb0ace769c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1781 batches | accuracy    0.350\n",
      "| epoch   1 |  1000/ 1781 batches | accuracy    0.703\n",
      "| epoch   1 |  1500/ 1781 batches | accuracy    0.824\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.80s | valid accuracy    0.875 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1781 batches | accuracy    0.862\n",
      "| epoch   2 |  1000/ 1781 batches | accuracy    0.874\n",
      "| epoch   2 |  1500/ 1781 batches | accuracy    0.884\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 14.38s | valid accuracy    0.900 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1781 batches | accuracy    0.901\n",
      "| epoch   3 |  1000/ 1781 batches | accuracy    0.900\n",
      "| epoch   3 |  1500/ 1781 batches | accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 14.28s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1781 batches | accuracy    0.915\n",
      "| epoch   4 |  1000/ 1781 batches | accuracy    0.912\n",
      "| epoch   4 |  1500/ 1781 batches | accuracy    0.915\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 14.39s | valid accuracy    0.919 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "295DLa8h6mGJ",
    "outputId": "081bf47c-695b-4e6f-ff8c-febba71b6579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.916\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uklCgZUd6mi1",
    "outputId": "d91b9dc2-0cca-480f-d0cb-6eaf69aa7bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text)).unsqueeze(1)\n",
    "        output = model(text)\n",
    "        return output[-1].argmax(1).item() + 1\n",
    "\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, we utilized only half of the transformer model, specifically the encoder. In this tutorial, we'll delve into how to set up a sequence-to-sequence model by making full use of both the encoder and decoder components of the transformer. Let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 embedding_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 d_hid: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=embedding_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=d_hid,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(embedding_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, embedding_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            embedding_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: torch.Tensor,\n",
    "                trg: torch.Tensor,\n",
    "                src_mask: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor,\n",
    "                src_padding_mask: torch.Tensor,\n",
    "                tgt_padding_mask: torch.Tensor,\n",
    "                memory_key_padding_mask: torch.Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqTransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=10000, bias=True)\n",
      "  (src_tok_emb): Embedding(10000, 512)\n",
      "  (tgt_tok_emb): Embedding(10000, 512)\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = 10000 #len(vocab_src)\n",
    "TGT_VOCAB_SIZE = 10000 #len(vocab_tgt)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 64\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* [``LANGUAGE TRANSLATION WITH NN.TRANSFORMER AND TORCHTEXT``](https://pytorch.org/tutorials/beginner/translation_transformer.html)\n",
    "* [``LANGUAGE MODELING WITH NN.TRANSFORMER AND TORCHTEXT``](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbfSzQywDKrzZmgQXXPBIv",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
